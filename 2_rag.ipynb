{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Introduction au RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "## Objectifs p√©dagogiques\n",
        "\n",
        "Dans ce notebook, vous allez d√©couvrir :\n",
        "- üî§ **Les embeddings** : comment transformer du texte en vecteurs\n",
        "- üìè **La similarit√© cosinus** : mesurer la proximit√© entre textes\n",
        "- üìÑ **Le chunking** : d√©couper des documents longs\n",
        "- üîç **La recherche vectorielle** avec FAISS\n",
        "- üí¨ **Le RAG** : combiner recherche et g√©n√©ration de texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Installation des d√©pendances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation des packages n√©cessaires\n",
        "!pip install -q transformers torch faiss-cpu langchain langchain-community sentence-transformers langgraph langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports n√©cessaires\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Toutes les d√©pendances sont install√©es !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî§ Partie 1 : Comprendre les Embeddings\n",
        "\n",
        "Les embeddings transforment du texte en vecteurs num√©riques qui capturent le sens du texte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIGURATION - Modifiez ces param√®tres !\n",
        "# Essayez diff√©rents mod√®les d'embedding\n",
        "EMBEDDING_MODEL_CONFIG = {\n",
        "    # Mod√®le actuel (modifiez pour tester)\n",
        "    \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Petit et rapide (384 dimensions)\n",
        "    # Autres options √† essayer :\n",
        "    # \"sentence-transformers/all-mpnet-base-v2\"  # Plus pr√©cis (768 dimensions)\n",
        "    # \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"  # Multilingue (384 dimensions)\n",
        "}\n",
        "\n",
        "# Chargement du mod√®le\n",
        "print(f\"üîÑ Chargement du mod√®le : {EMBEDDING_MODEL_CONFIG['model_name']}\")\n",
        "embedder = SentenceTransformer(EMBEDDING_MODEL_CONFIG['model_name'])\n",
        "print(f\"‚úÖ Mod√®le charg√© ! Dimension des embeddings : {embedder.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemples de phrases √† encoder\n",
        "phrases_test = [\n",
        "    \"Le chat dort sur le canap√©\",\n",
        "    \"Le f√©lin se repose sur le sofa\",\n",
        "    \"La voiture roule sur l'autoroute\",\n",
        "    \"Python est un langage de programmation\"\n",
        "]\n",
        "\n",
        "# G√©n√©ration des embeddings\n",
        "embeddings = embedder.encode(phrases_test)\n",
        "\n",
        "print(\"üìä Analyse des embeddings :\")\n",
        "for i, phrase in enumerate(phrases_test):\n",
        "    print(f\"\\nPhrase {i+1}: '{phrase}'\")\n",
        "    print(f\"  - Dimension de l'embedding : {embeddings[i].shape}\")\n",
        "    print(f\"  - Premiers √©l√©ments : {embeddings[i][:5].round(3)}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 1 : Analyse des dimensions\n",
        "\n",
        "**R√©pondez dans la cellule ci-dessous :**\n",
        "- Quelle est la dimension des vecteurs d'embedding ?\n",
        "- Pourquoi pensez-vous que des mod√®les diff√©rents ont des dimensions diff√©rentes ?\n",
        "- Quel pourrait √™tre l'impact de la dimension sur les performances ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Votre r√©ponse :**\n",
        "\n",
        "[√âcrivez votre r√©ponse ici]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìè Partie 2 : Similarit√© Cosinus\n",
        "\n",
        "La similarit√© cosinus mesure √† quel point deux vecteurs pointent dans la m√™me direction (de -1 √† 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcul de la matrice de similarit√©\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Affichage des r√©sultats\n",
        "print(\"üìä Matrice de similarit√© cosinus :\\n\")\n",
        "print(\"     \", end=\"\")\n",
        "for i in range(len(phrases_test)):\n",
        "    print(f\"  P{i+1}  \", end=\"\")\n",
        "print()\n",
        "\n",
        "for i in range(len(phrases_test)):\n",
        "    print(f\"P{i+1}: \", end=\"\")\n",
        "    for j in range(len(phrases_test)):\n",
        "        sim = similarity_matrix[i][j]\n",
        "        print(f\"{sim:.3f} \", end=\"\")\n",
        "    print()\n",
        "\n",
        "# Interpr√©tation\n",
        "print(\"\\nüîç Interpr√©tation des similarit√©s :\")\n",
        "for i in range(len(phrases_test)):\n",
        "    for j in range(i+1, len(phrases_test)):\n",
        "        sim = similarity_matrix[i][j]\n",
        "        print(f\"\\n'{phrases_test[i]}' <-> '{phrases_test[j]}'\")\n",
        "        print(f\"  Similarit√© : {sim:.3f} - \", end=\"\")\n",
        "        if sim > 0.8:\n",
        "            print(\"‚úÖ Tr√®s similaire\")\n",
        "        elif sim > 0.5:\n",
        "            print(\"üî∂ Moyennement similaire\")\n",
        "        else:\n",
        "            print(\"‚ùå Peu similaire\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCICE : Testez vos propres phrases !\n",
        "# Modifiez ces phrases pour explorer la similarit√©\n",
        "MES_PHRASES = [\n",
        "    \"Le machine learning est fascinant\",\n",
        "    \"L'apprentissage automatique est passionnant\",\n",
        "    \"J'aime manger des pizzas\",\n",
        "    \"L'intelligence artificielle r√©volutionne le monde\"\n",
        "]\n",
        "\n",
        "# Calcul des embeddings et similarit√©s\n",
        "mes_embeddings = embedder.encode(MES_PHRASES)\n",
        "mes_similarities = cosine_similarity(mes_embeddings)\n",
        "\n",
        "# Affichage\n",
        "print(\"üéØ Vos phrases et leurs similarit√©s :\\n\")\n",
        "for i in range(len(MES_PHRASES)):\n",
        "    for j in range(i+1, len(MES_PHRASES)):\n",
        "        sim = mes_similarities[i][j]\n",
        "        print(f\"'{MES_PHRASES[i]}' <-> '{MES_PHRASES[j]}'\")\n",
        "        print(f\"  ‚Üí Similarit√© : {sim:.3f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 2 : Analyse de la similarit√©\n",
        "\n",
        "**R√©pondez dans la cellule ci-dessous :**\n",
        "- Quelles paires de phrases ont la plus haute similarit√© ? Pourquoi ?\n",
        "- La similarit√© capture-t-elle bien le sens s√©mantique ?\n",
        "- Que se passe-t-il avec des synonymes vs des mots diff√©rents ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Votre r√©ponse :**\n",
        "\n",
        "[√âcrivez votre r√©ponse ici]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÑ Partie 3 : Chunking de Documents\n",
        "\n",
        "Les mod√®les ont des limites de contexte. Il faut d√©couper les documents longs en morceaux (chunks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Documents sources pour notre base de connaissances\n",
        "DOCUMENTS = [\n",
        "    {\n",
        "        \"titre\": \"Introduction √† Python\",\n",
        "        \"contenu\": \"\"\"Python est un langage de programmation interpr√©t√©, de haut niveau et √† usage g√©n√©ral. \n",
        "        Cr√©√© par Guido van Rossum et publi√© pour la premi√®re fois en 1991, Python a une philosophie de conception \n",
        "        qui met l'accent sur la lisibilit√© du code, notamment en utilisant des espaces blancs significatifs. \n",
        "        Il fournit des constructions qui permettent une programmation claire √† petite et grande √©chelle. \n",
        "        Python dispose d'un syst√®me de type dynamique et d'une gestion automatique de la m√©moire. \n",
        "        Il prend en charge plusieurs paradigmes de programmation, notamment la programmation proc√©durale, \n",
        "        orient√©e objet et fonctionnelle. Python dispose d'une biblioth√®que standard compl√®te et vari√©e.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"titre\": \"Le Machine Learning\",\n",
        "        \"contenu\": \"\"\"Le Machine Learning est une branche de l'intelligence artificielle qui permet aux syst√®mes \n",
        "        d'apprendre et de s'am√©liorer automatiquement √† partir de l'exp√©rience sans √™tre explicitement programm√©s. \n",
        "        Le ML se concentre sur le d√©veloppement de programmes informatiques qui peuvent acc√©der aux donn√©es \n",
        "        et les utiliser pour apprendre par eux-m√™mes. Le processus d'apprentissage commence par des observations \n",
        "        ou des donn√©es, afin de rechercher des mod√®les dans les donn√©es et de prendre de meilleures d√©cisions \n",
        "        √† l'avenir. L'objectif principal est de permettre aux ordinateurs d'apprendre automatiquement sans \n",
        "        intervention humaine et d'ajuster leurs actions en cons√©quence.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"titre\": \"Les R√©seaux de Neurones\",\n",
        "        \"contenu\": \"\"\"Les r√©seaux de neurones artificiels sont des syst√®mes informatiques inspir√©s des r√©seaux \n",
        "        de neurones biologiques qui constituent le cerveau animal. Ces r√©seaux sont bas√©s sur une collection \n",
        "        d'unit√©s connect√©es appel√©es neurones artificiels, qui mod√©lisent vaguement les neurones biologiques. \n",
        "        Chaque connexion peut transmettre un signal d'un neurone √† un autre. Un neurone artificiel qui re√ßoit \n",
        "        un signal peut le traiter et signaler ensuite les neurones qui lui sont connect√©s. Dans les impl√©mentations \n",
        "        courantes, le signal est un nombre r√©el et la sortie de chaque neurone est calcul√©e par une fonction \n",
        "        non lin√©aire de la somme de ses entr√©es.\"\"\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIGURATION DU CHUNKING - Modifiez ces param√®tres !\n",
        "CHUNKING_CONFIG = {\n",
        "    \"chunk_size\": 200,  # Taille des chunks en caract√®res (essayez 100, 200, 500)\n",
        "    \"chunk_overlap\": 50,  # Chevauchement entre chunks (essayez 0, 50, 100)\n",
        "    \"separators\": [\"\\n\\n\", \"\\n\", \".\", \",\", \" \"]  # S√©parateurs pour d√©couper\n",
        "}\n",
        "\n",
        "# Cr√©ation du text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNKING_CONFIG[\"chunk_size\"],\n",
        "    chunk_overlap=CHUNKING_CONFIG[\"chunk_overlap\"],\n",
        "    separators=CHUNKING_CONFIG[\"separators\"]\n",
        ")\n",
        "\n",
        "# Application du chunking\n",
        "print(f\"üìÑ Configuration du chunking :\")\n",
        "print(f\"  - Taille des chunks : {CHUNKING_CONFIG['chunk_size']} caract√®res\")\n",
        "print(f\"  - Chevauchement : {CHUNKING_CONFIG['chunk_overlap']} caract√®res\\n\")\n",
        "\n",
        "all_chunks = []\n",
        "for doc in DOCUMENTS:\n",
        "    chunks = text_splitter.split_text(doc[\"contenu\"])\n",
        "    print(f\"\\nüìë Document : {doc['titre']}\")\n",
        "    print(f\"  - Longueur originale : {len(doc['contenu'])} caract√®res\")\n",
        "    print(f\"  - Nombre de chunks : {len(chunks)}\")\n",
        "    \n",
        "    for i, chunk in enumerate(chunks):\n",
        "        all_chunks.append({\n",
        "            \"source\": doc[\"titre\"],\n",
        "            \"chunk_id\": i,\n",
        "            \"content\": chunk\n",
        "        })\n",
        "        print(f\"\\n  Chunk {i+1} ({len(chunk)} caract√®res) :\")\n",
        "        print(f\"  '{chunk[:80]}...'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 3 : Strat√©gies de chunking\n",
        "\n",
        "**Exp√©rimentez en modifiant les param√®tres puis r√©pondez :**\n",
        "- Que se passe-t-il quand vous augmentez/diminuez la taille des chunks ?\n",
        "- Quel est l'effet du chevauchement (overlap) ?\n",
        "- Quels sont les avantages/inconv√©nients de chunks petits vs grands ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Votre r√©ponse :**\n",
        "\n",
        "[√âcrivez votre r√©ponse ici]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Partie 4 : Recherche Vectorielle avec FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr√©ation de la base vectorielle FAISS\n",
        "print(\"üî® Construction de la base vectorielle FAISS...\")\n",
        "\n",
        "# Cr√©ation des embeddings pour tous les chunks\n",
        "texts = [chunk[\"content\"] for chunk in all_chunks]\n",
        "metadatas = [{\"source\": chunk[\"source\"], \"chunk_id\": chunk[\"chunk_id\"]} for chunk in all_chunks]\n",
        "\n",
        "# Initialisation du mod√®le d'embeddings pour Langchain\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_CONFIG[\"model_name\"]\n",
        ")\n",
        "\n",
        "# Cr√©ation du vectorstore FAISS\n",
        "vectorstore = FAISS.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embeddings_model,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Base vectorielle cr√©√©e avec {len(texts)} chunks !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIGURATION DE LA RECHERCHE - Modifiez ces param√®tres !\n",
        "SEARCH_CONFIG = {\n",
        "    \"k\": 3,  # Nombre de r√©sultats √† retourner (essayez 1, 3, 5)\n",
        "    \"search_type\": \"similarity\",  # Type de recherche\n",
        "}\n",
        "\n",
        "# Questions de test\n",
        "QUESTIONS_TEST = [\n",
        "    \"Qu'est-ce que Python ?\",\n",
        "    \"Comment fonctionne l'apprentissage automatique ?\",\n",
        "    \"Qu'est-ce qu'un neurone artificiel ?\",\n",
        "    \"Qui a cr√©√© Python ?\",\n",
        "]\n",
        "\n",
        "# Test de recherche\n",
        "print(f\"üîç Test de recherche (top-{SEARCH_CONFIG['k']} r√©sultats)\\n\")\n",
        "\n",
        "for question in QUESTIONS_TEST:\n",
        "    print(f\"\\n‚ùì Question : '{question}'\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Recherche\n",
        "    results = vectorstore.similarity_search_with_score(\n",
        "        question, \n",
        "        k=SEARCH_CONFIG[\"k\"]\n",
        "    )\n",
        "    \n",
        "    # Affichage des r√©sultats\n",
        "    for i, (doc, score) in enumerate(results):\n",
        "        print(f\"\\nüìÑ R√©sultat {i+1} (score: {score:.3f})\")\n",
        "        print(f\"   Source : {doc.metadata['source']} - Chunk {doc.metadata['chunk_id']}\")\n",
        "        print(f\"   Contenu : '{doc.page_content[:100]}...'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 4 : Analyse de la recherche\n",
        "\n",
        "**Testez diff√©rentes valeurs de k puis r√©pondez :**\n",
        "- Les r√©sultats retourn√©s sont-ils pertinents ?\n",
        "- Comment le nombre de r√©sultats (k) affecte-t-il la qualit√© ?\n",
        "- Que se passe-t-il si la question est ambigu√´ ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Votre r√©ponse :**\n",
        "\n",
        "[√âcrivez votre r√©ponse ici]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Partie 5 : Construction du syst√®me RAG complet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement du mod√®le de g√©n√©ration\n",
        "print(\"ü§ñ Chargement du mod√®le de chat...\")\n",
        "\n",
        "# CONFIGURATION DU MOD√àLE - Modifiez ce param√®tre !\n",
        "GENERATION_MODEL = \"microsoft/DialoGPT-small\"  # Mod√®le l√©ger pour Colab\n",
        "# Autres options possibles :\n",
        "# \"google/flan-t5-small\"  # Mod√®le T5 pour Q&A\n",
        "# \"facebook/blenderbot-400M-distill\"  # Mod√®le de dialogue\n",
        "\n",
        "# Chargement avec pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=GENERATION_MODEL,\n",
        "    max_length=200,\n",
        "    temperature=0.7,\n",
        "    pad_token_id=50256\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Mod√®le {GENERATION_MODEL} charg√© !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction RAG simple\n",
        "def rag_query(question, k=3, verbose=True):\n",
        "    \"\"\"\n",
        "    Fonction RAG compl√®te :\n",
        "    1. Recherche les documents pertinents\n",
        "    2. Construit un contexte\n",
        "    3. G√©n√®re une r√©ponse\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\nüîç Recherche pour : '{question}'\")\n",
        "    \n",
        "    # √âtape 1 : Recherche\n",
        "    docs = vectorstore.similarity_search(question, k=k)\n",
        "    \n",
        "    # √âtape 2 : Construction du contexte\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nüìö Contexte trouv√© ({len(docs)} documents) :\")\n",
        "        for i, doc in enumerate(docs):\n",
        "            print(f\"  - Doc {i+1}: {doc.metadata['source']}\")\n",
        "    \n",
        "    # √âtape 3 : G√©n√©ration\n",
        "    prompt = f\"\"\"Contexte : {context}\n",
        "    \n",
        "Question : {question}\n",
        "    \n",
        "R√©ponse bas√©e sur le contexte : \"\"\"\n",
        "    \n",
        "    response = generator(prompt, max_length=300, do_sample=True)[0]['generated_text']\n",
        "    \n",
        "    # Extraction de la r√©ponse\n",
        "    answer = response.split(\"R√©ponse bas√©e sur le contexte : \")[-1].strip()\n",
        "    \n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"context\": context,\n",
        "        \"answer\": answer,\n",
        "        \"sources\": [doc.metadata for doc in docs]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TESTEZ LE SYST√àME RAG !\n",
        "# Modifiez ces questions pour explorer\n",
        "MES_QUESTIONS_RAG = [\n",
        "    \"Qu'est-ce que Python et qui l'a cr√©√© ?\",\n",
        "    \"Comment fonctionne un r√©seau de neurones ?\",\n",
        "    \"Quelle est la diff√©rence entre ML et les r√©seaux de neurones ?\",\n",
        "]\n",
        "\n",
        "# Configuration\n",
        "K_DOCUMENTS = 2  # Nombre de documents √† r√©cup√©rer (modifiez !)\n",
        "\n",
        "# Test du RAG\n",
        "for question in MES_QUESTIONS_RAG:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    result = rag_query(question, k=K_DOCUMENTS, verbose=True)\n",
        "    \n",
        "    print(f\"\\nüí¨ R√©ponse g√©n√©r√©e :\")\n",
        "    print(f\"{result['answer']}\")\n",
        "    \n",
        "    print(f\"\\nüìå Sources utilis√©es :\")\n",
        "    for source in result['sources']:\n",
        "        print(f\"  - {source['source']} (chunk {source['chunk_id']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Partie 6 : Utilisation de LangGraph pour orchestrer le RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, List, Dict\n",
        "\n",
        "# D√©finition de l'√©tat\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    context: str\n",
        "    documents: List[Dict]\n",
        "    answer: str\n",
        "    k: int\n",
        "\n",
        "# Fonctions pour chaque n≈ìud\n",
        "def retrieve_documents(state: RAGState) -> RAGState:\n",
        "    \"\"\"R√©cup√®re les documents pertinents\"\"\"\n",
        "    print(\"üìö √âtape 1: Recherche de documents...\")\n",
        "    docs = vectorstore.similarity_search(state[\"question\"], k=state[\"k\"])\n",
        "    \n",
        "    state[\"documents\"] = [\n",
        "        {\"content\": doc.page_content, \"metadata\": doc.metadata} \n",
        "        for doc in docs\n",
        "    ]\n",
        "    state[\"context\"] = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    \n",
        "    print(f\"  ‚Üí {len(docs)} documents trouv√©s\")\n",
        "    return state\n",
        "\n",
        "def generate_answer(state: RAGState) -> RAGState:\n",
        "    \"\"\"G√©n√®re la r√©ponse bas√©e sur le contexte\"\"\"\n",
        "    print(\"ü§ñ √âtape 2: G√©n√©ration de la r√©ponse...\")\n",
        "    \n",
        "    prompt = f\"\"\"Contexte : {state['context']}\n",
        "    \n",
        "Question : {state['question']}\n",
        "    \n",
        "R√©ponse : \"\"\"\n",
        "    \n",
        "    # Simulation de g√©n√©ration (remplacez par votre mod√®le)\n",
        "    # Pour simplifier, on utilise une r√©ponse basique\n",
        "    if \"Python\" in state[\"question\"]:\n",
        "        state[\"answer\"] = \"D'apr√®s le contexte, Python est un langage de programmation cr√©√© par Guido van Rossum en 1991.\"\n",
        "    elif \"r√©seau\" in state[\"question\"] or \"neurone\" in state[\"question\"]:\n",
        "        state[\"answer\"] = \"Les r√©seaux de neurones sont des syst√®mes inspir√©s du cerveau, compos√©s de neurones artificiels connect√©s.\"\n",
        "    else:\n",
        "        state[\"answer\"] = \"D'apr√®s les documents trouv√©s, \" + state[\"context\"][:150] + \"...\"\n",
        "    \n",
        "    print(\"  ‚Üí R√©ponse g√©n√©r√©e\")\n",
        "    return state\n",
        "\n",
        "# Construction du graphe\n",
        "workflow = StateGraph(RAGState)\n",
        "\n",
        "# Ajout des n≈ìuds\n",
        "workflow.add_node(\"retrieve\", retrieve_documents)\n",
        "workflow.add_node(\"generate\", generate_answer)\n",
        "\n",
        "# D√©finition du flux\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compilation\n",
        "rag_app = workflow.compile()\n",
        "\n",
        "print(\"‚úÖ Graphe LangGraph cr√©√© !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test du syst√®me RAG avec LangGraph\n",
        "print(\"üöÄ Test du RAG avec LangGraph\\n\")\n",
        "\n",
        "# CONFIGURATION - Modifiez ces param√®tres !\n",
        "test_queries = [\n",
        "    {\"question\": \"Explique-moi Python\", \"k\": 2},\n",
        "    {\"question\": \"Comment apprennent les machines ?\", \"k\": 3},\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚ùì Question : {query['question']}\")\n",
        "    print(f\"üìä Param√®tres : k={query['k']}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Ex√©cution du workflow\n",
        "    result = rag_app.invoke(query)\n",
        "    \n",
        "    print(f\"\\nüí¨ R√©ponse finale : {result['answer']}\")\n",
        "    print(f\"\\nüìå Documents utilis√©s :\")\n",
        "    for doc in result['documents']:\n",
        "        print(f\"  - {doc['metadata']['source']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§î Question 5 : Analyse du syst√®me RAG complet\n",
        "\n",
        "**Apr√®s avoir test√© le syst√®me, r√©pondez :**\n",
        "- Quels sont les avantages d'utiliser un syst√®me RAG vs un LLM seul ?\n",
        "- Comment le nombre de documents r√©cup√©r√©s (k) affecte-t-il la r√©ponse ?\n",
        "- Quelles am√©liorations pourriez-vous sugg√©rer ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Votre r√©ponse :**\n",
        "\n",
        "[√âcrivez votre r√©ponse ici]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Exp√©rience finale : Comparaison des configurations\n",
        "\n",
        "Testez diff√©rentes configurations pour comprendre leur impact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXP√âRIENCE : Modifiez ces configurations !\n",
        "EXPERIMENTS = [\n",
        "    {\n",
        "        \"name\": \"Config 1 : Chunks petits, peu de docs\",\n",
        "        \"chunk_size\": 100,\n",
        "        \"chunk_overlap\": 20,\n",
        "        \"k_documents\": 1\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Config 2 : Chunks moyens, plus de docs\",\n",
        "        \"chunk_size\": 300,\n",
        "        \"chunk_overlap\": 50,\n",
        "        \"k_documents\": 3\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Config 3 : Grands chunks, beaucoup d'overlap\",\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 150,\n",
        "        \"k_documents\": 2\n",
        "    }\n",
        "]\n",
        "\n",
        "TEST_QUESTION = \"Comment Python g√®re-t-il la m√©moire ?\"\n",
        "\n",
        "print(f\"üß™ Test de la question : '{TEST_QUESTION}'\\n\")\n",
        "\n",
        "for config in EXPERIMENTS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîß {config['name']}\")\n",
        "    print(f\"  - Chunk size: {config['chunk_size']}\")\n",
        "    print(f\"  - Overlap: {config['chunk_overlap']}\")\n",
        "    print(f\"  - K documents: {config['k_documents']}\")\n",
        "    \n",
        "    # Note : Dans un vrai test, vous recr√©eriez le vectorstore avec ces param√®tres\n",
        "    # Ici, on simule juste l'effet\n",
        "    print(f\"\\n  ‚Üí Impact attendu :\")\n",
        "    if config['chunk_size'] < 200:\n",
        "        print(\"    ‚Ä¢ Contexte plus pr√©cis mais potentiellement incomplet\")\n",
        "    else:\n",
        "        print(\"    ‚Ä¢ Contexte plus complet mais potentiellement moins focalis√©\")\n",
        "    \n",
        "    if config['k_documents'] > 2:\n",
        "        print(\"    ‚Ä¢ Plus d'information mais risque de bruit\")\n",
        "    else:\n",
        "        print(\"    ‚Ä¢ Information focalis√©e mais peut manquer des d√©tails\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì R√©capitulatif et points cl√©s\n",
        "\n",
        "### Ce que vous avez appris :\n",
        "\n",
        "1. **Embeddings** :\n",
        "   - Transformation de texte en vecteurs num√©riques\n",
        "   - Diff√©rents mod√®les = diff√©rentes dimensions\n",
        "   - Capture du sens s√©mantique\n",
        "\n",
        "2. **Similarit√© cosinus** :\n",
        "   - Mesure de proximit√© s√©mantique (0 √† 1)\n",
        "   - Fonctionne bien avec les synonymes\n",
        "   - Base de la recherche vectorielle\n",
        "\n",
        "3. **Chunking** :\n",
        "   - N√©cessaire pour les limites de contexte\n",
        "   - Trade-off : pr√©cision vs compl√©tude\n",
        "   - L'overlap aide √† maintenir le contexte\n",
        "\n",
        "4. **FAISS** :\n",
        "   - Recherche efficace dans l'espace vectoriel\n",
        "   - Permet de retrouver rapidement les documents pertinents\n",
        "\n",
        "5. **RAG** :\n",
        "   - Combine recherche et g√©n√©ration\n",
        "   - Plus pr√©cis qu'un LLM seul\n",
        "   - Permet d'utiliser des connaissances sp√©cifiques\n",
        "\n",
        "### üí° Conseils pour optimiser un syst√®me RAG :\n",
        "\n",
        "- **Qualit√© des embeddings** : Testez diff√©rents mod√®les\n",
        "- **Strat√©gie de chunking** : Adaptez √† votre contenu\n",
        "- **Nombre de documents (k)** : √âquilibre pr√©cision/bruit\n",
        "- **Prompt engineering** : Guidez bien le mod√®le de g√©n√©ration\n",
        "- **M√©tadonn√©es** : Utilisez-les pour filtrer/classer\n",
        "\n",
        "### üöÄ Pour aller plus loin :\n",
        "\n",
        "- Essayez d'autres mod√®les d'embedding (multilingues, domaine-sp√©cifique)\n",
        "- Explorez des strat√©gies de chunking avanc√©es (s√©mantique, par paragraphe)\n",
        "- Testez diff√©rents algorithmes FAISS (IVF, HNSW)\n",
        "- Impl√©mentez du re-ranking des r√©sultats\n",
        "- Ajoutez de la m√©moire conversationnelle"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
